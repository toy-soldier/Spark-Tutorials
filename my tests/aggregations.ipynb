{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2571dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "DIRPATH = \"C:/Users/sanjay s risbud/Dropbox/My Work/practice/python/pyspark/Spark-Tutorials-master/data/\"\n",
    "spark = SparkSession.builder.appName(\"Exercise\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9efd8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(DIRPATH + \"sales_info.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1c7b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8172b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df[\"Company\"]).max(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9ccc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarize using agg()\n",
    "df.agg({\"Sales\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7afc0ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same as above\n",
    "df.groupBy(df[\"Company\"]).agg({\"Sales\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bfe0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql functions\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0878120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-------+-----------+\n",
      "|Company|Min Sales|Max Sales|Average|Total Sales|\n",
      "+-------+---------+---------+-------+-----------+\n",
      "|   APPL|   130.00|   750.00| 370.00|   1,480.00|\n",
      "|   GOOG|   120.00|   340.00| 220.00|     660.00|\n",
      "|     FB|   350.00|   870.00| 610.00|   1,220.00|\n",
      "|   MSFT|   124.00|   600.00| 322.33|     967.00|\n",
      "+-------+---------+---------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, count, avg, sum, col, format_number\n",
    "sales_column = col(\"Sales\")\n",
    "columns_to_display = [\n",
    "    format_number(min(sales_column), 2).alias(\"Min Sales\"),\n",
    "    format_number(max(sales_column), 2).alias(\"Max Sales\"),\n",
    "    format_number(avg(sales_column), 2).alias(\"Average\"),\n",
    "    format_number(sum(sales_column), 2).alias(\"Total Sales\"),\n",
    "]\n",
    "result = df.groupBy(\"Company\").agg(*columns_to_display)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa52e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df[\"Company\"].asc(), df[\"Sales\"].desc()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
